

# Spark核心概念简介



Spark提供了一个交互式的shell，可以作为即时的数据分析工具。类似于R,Python所提供额Shell，或者是操作系统所提供的Shell（Bash）。但是与操作系统的shell不同的是，操作系统上的shell只能够操作单机的硬盘或者是内存，Spark shell 可以与分布式存储在多机上的内存硬盘进行交互，并且数据处理的过程由Spark自动控制完成。

pySpark shell 开启命令 bin/pyspark

scala版本的shell开启命令 bin/spark-shell

退出命令 ctrl+D

 

在conf目录下可以，创建一个log4j.properties来使用Log.4j的日志库。目录下面已经有一个模板文件可以直接调用，log4j.properties.template.

 

我们通过 RDD的操作来表达我们的计算意图，这些自动在集群上进行并行计算机。

RDD是Spark对分布式数据和计算的抽象。



每个Spark应用，都由一个驱动器程序（Driver program）发起集群上的各种操作。驱动器内包含了应用的main函数，并且定义在集群上的分布式数据集，还对数据集应用了相关的操作。

Spark-Shell就是驱动器程序，你需要输入你想要的操作就好了。

驱动器通过一个SparkContext对象访问Spark.对象代表了对计算集群的一个连接，shell启动的时候已经自动创建一个SparkContext对象，sc变量就是了。

驱动程序一般要管理多个执行器excutor节点，比如我们在集群上所执行的count操作，那么不同的节点会统计文件的不同部分的行数。

spark-shell模式下，在本地模式上工作只会在一个节点上执行。

在独立的程序下，你要手动初始化SparkContext

##  创建独立的应用

```scala


import org.apache.spark.{SparkConf, SparkContext}

object MySpark {

  def main(args: Array[String]) {

​    val conf = new SparkConf().setAppName("mySpark")

​    //setMaster("local") 本机的spark就用local，远端的就写ip

​    //如果是打成jar包运行则需要去掉 setMaster("local")因为在参数中会指定。

​    conf.setMaster("local")

​    val sc =new SparkContext(conf)

​    val rdd =sc.parallelize(List(1,2,3,4,5,6)).map(_*3)

​    val mappedRDD=rdd.filter(_>10).collect()

​    //对集合求和

​    println(rdd.reduce(_+_))

​    //输出大于10的元素

​    for(arg <- mappedRDD)

​      print(arg+" ")

​    println()

​    println("math is work")

  }

}
```

 集群URL：告诉Spark如何连接到集群之上，在这个集群中使用的是local,这个可以告诉Spark单机单线程上运行，而不是连接到一个集群上。

 注意集群管理器的用户界面。SparkUI: Bound SparkUI 

 一般使用maven或者是sbt创建独立的应用。

RDD编程（基本操作）

Resilient Distributed Dataset 弹性分布式数据集

RDD支持转换（transformation）和行动操作(action)

转化操作：RDD的转化操作是返回一个新的RDD的操作，比如map()和filter

行动操作：是向驱动器返回结果，或者把结果写入外部系统的操作会触发实际的计算，比如count()或者first()。

1.RDD是惰性求值的。

2.filter是操作不会修改原来的rdd，会返回一个全新的rdd

转换出来的RDD操作，是从已有的RDD中派生出来的，Spark会用谱系图记录（lineage graph）来记录这些不同RDD之间的转化。

可以依靠系谱图在持久化的RDD丢失的部分数据时恢复所丢失的数据。

collect()获取整个RDD的数据，一般数据都很大，单台机器的内存放不下，一般讲数据放入HDFS,Amazon S3这样的分布式存储系统中。

take（）获取部分数据

1.伪集合操作

distrinct()将所有数据通过网络进行混洗

union(others) 会包含重复的元素

intersect(other)返回共有的，会通过网络进行数据混洗，发现共同数据。

A.subtract(B)==A-B

A.Catesian（B）=A*B笛卡尔积

sample()对RDD进行采样 

2.行动操作

reduce

3.不同类型RDD的转换



## 持久化存储

sparkRDD是惰性求值的，有时候需要重复使用同一个RDD，如果简单地对RDD调用行动操作，Spark每次回重算RDD以及它的依赖。这在迭代算法中，消耗格外的大。

为了避免多次计算一个RDD可以，让spark对数据进行持久化。持久化存储一个RDD时，计算出RDD的节点会分别保存它们所求出来的分区数据。

如果一个持久化的分区节点发生故障，spark会需要用到缓存的数据重新计算数据分区。

 出于不同目的，我们可以为RDD选择不同的持久化级别，在scala和java中，默认的情况下，persist（）会把数据以序列化的形式存储在JVM的堆空间中。

在把数据储存到磁盘或者是堆外存储上时，也总是使用序列化后的数据。

 如果缓存的数据太多了，内存中放不下，Spark会利用最近最少使用LRU的缓存策略，把最古老的分区从内存中移出。

 对于仅仅把数据存放到内存中缓存级别，如果下次用到已经被移除的分区时，这些分区就要重新计算了。

对于内存和磁盘的缓存级别来说，被移除的分区会写入磁盘。

但是无论哪种级别，你都不必担心



用vchar存是可以的。但是首先对数据库的读/写的速度永远都赶不上文件系统处理的速度，其次[数据库备份](https://www.baidu.com/s?wd=数据库备份&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)变的巨大，越来越耗时间，最后对文件的访问需要穿越你的应用层和数据库层。图片是数据库最大的杀手。还是存放一个url然后在调用吧！

 

反正图片，文件，二进制数这三样东西永远别存数据库。